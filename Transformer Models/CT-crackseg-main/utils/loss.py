import torch
import torch.nn as nn
import torch.nn.functional as F

class BinaryFocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, logits=False, size_average=True):
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.size_average = size_average
        self.criterion = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, inputs, targets):
        BCE_loss = self.criterion(inputs, targets)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.size_average:
            return F_loss.mean()
        else:
            return F_loss.sum()
        
class BinaryFocalLoss1(nn.Module): 
    def __init__(self, alpha=1, gamma=2, logits=False, size_average=True, w0 = 1, w1 = 1):
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.size_average = size_average
        self.w0 = w0 # the weight of noncrack pixels
        self.w1 = w1 # the weight of crack pixels
        if self.w0 == None or self.w1==None:
            raise ValueError("w must be a number")

    def forward(self, inputs, targets):
        weight=torch.zeros_like(targets)
        weight=torch.fill_(weight, self.w0)
        weight[targets>0]=self.w1
        BCE_loss = nn.BCEWithLogitsLoss(weight=weight, reduction='none')(inputs, targets)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.size_average:
            return F_loss.mean()
        else:
            return F_loss.sum()


class MyBCELoss(nn.Module):

    def __init__(self, w0 = None, w1 = None):
        super(MyBCELoss, self).__init__()
        self.w0 = w0 # the weight of noncrack pixels
        self.w1 = w1 # the weight of crack pixels
        if self.w0 == None or self.w1==None:
            raise ValueError("w must be a number")
        
    def forward(self, input, target):
        weight=torch.zeros_like(target)
        weight=torch.fill_(weight, self.w0)
        weight[target>0]=self.w1
        loss=nn.BCELoss(weight=weight,reduction='mean')(input,target)
        return loss

class FocalLossV1(nn.Module):

    def __init__(self,
                 alpha=0.25,
                 gamma=2,
                 reduction='mean',):
        super(FocalLossV1, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.crit = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, logits, label):
        '''
        Usage is same as nn.BCEWithLogits:
            >>> criteria = FocalLossV1()
            >>> logits = torch.randn(8, 19, 384, 384)
            >>> lbs = torch.randint(0, 2, (8, 19, 384, 384)).float()
            >>> loss = criteria(logits, lbs)
        '''
        probs = torch.sigmoid(logits)
        coeff = torch.abs(label - probs).pow(self.gamma).neg()
        log_probs = torch.where(logits >= 0,
                F.softplus(logits, -1, 50),
                logits - F.softplus(logits, 1, 50))
        log_1_probs = torch.where(logits >= 0,
                -logits + F.softplus(logits, -1, 50),
                -F.softplus(logits, 1, 50))
        loss = label * self.alpha * log_probs + (1. - label) * (1. - self.alpha) * log_1_probs
        loss = loss * coeff

        if self.reduction == 'mean':
            loss = loss.mean()
        if self.reduction == 'sum':
            loss = loss.sum()
        return 
    
def dice_loss(target,predictive,ep=1e-8):
    intersection = 2 * torch.sum(predictive * target) + ep
    union = torch.sum(predictive) + torch.sum(target) + ep
    loss = 1 - intersection / union
    return loss
 
class SoftDiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(SoftDiceLoss, self).__init__()
 
    def forward(self, logits, targets):
        bs = targets.size(0)
        smooth = 1
        
        probs = F.sigmoid(logits)
        m1 = probs.view(bs, -1)
        m2 = targets.view(bs, -1)
        intersection = (m1 * m2)
 
        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)
        score = 1 - score.sum() / bs
        return score

class DiceBCELoss(nn.Module):
    def __init__(self):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = torch.sigmoid(inputs)      
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        weight=torch.zeros_like(targets)
        weight=torch.fill_(weight, 0.04)
        weight[targets>0]=0.96
        
        intersection = (inputs * targets).sum()                            
        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean', weight=weight)
        Dice_BCE = BCE + dice_loss
        
        return Dice_BCE
    
class Custom_DiceBCELoss(nn.Module):
    def __init__(self):
        super(DiceBCELoss, self).__init__()
        self.bce_loss = nn.BCEWithLogitsLoss()

    def forward(self, inputs, targets, smooth=1):
        # inputs: [batch, num_classes, height, width]
        # targets: [batch, height, width] with class indices (0 to num_classes-1)

        inputs = torch.sigmoid(inputs)
        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1])  # Convert to one-hot encoding
        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # [batch, num_classes, height, width]

        # BCE Loss
        bce = self.bce_loss(inputs, targets_one_hot)

        # Dice Loss
        intersection = (inputs * targets_one_hot).sum(dim=(2, 3))
        cardinality = (inputs + targets_one_hot).sum(dim=(2, 3))
        dice_loss = (2. * intersection + smooth) / (cardinality + smooth)
        dice_loss = 1 - dice_loss.mean()

        return dice_loss + bce